### 操作系统

#### 1、什么是操作系统？

1. **操作系统(Operating System，简称OS)是管理计算机硬件与软件资源的程序，是计算机的基石。**
2. **操作系统本质上是一个运行在计算机上的软件程序，用于管理计算机硬件和软件资源。**举例：运行在你电脑上的所有应用程序都通过操作系统来调用系统内存以及磁盘等等硬件。
3. **操作系统存在屏蔽了硬件层的复杂性。**操作系统就像是硬件使用的负责人，统筹着各种相关事项。
4. **操作系统的内核(Kernel)是操作系统的核心部分，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。**内核是连接应用程序和硬件的桥梁，决定着系统的性能和稳定性。

![image-20210411135824915](https://gitee.com/yun-xiaojie/blog-image/raw/master/img/image-20210411135824915.png)

#### 2、系统调用

##### 用户态和内核态

根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别：

- **用户态(user mode)**：用户态运行的进程可以直接读取用户程序的数据。
- **内核态(kernel mode)**：可以简单的理解为系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。

**权限控制：**用户进程是受限的，它不能随意访问资源、获取资源。所以，由内核进程负责管理和分配资源，它具有最高权限，而用户进程使用被分配的资源。

**从用户态到内核态切换可以通过三种方式：**

1. 系统调用。其实系统调用本身就是中断，但是软件中断，跟硬中断不同。
2. 异常：如果当前进程运行在用户态，如果这个时候发生了异常事件，就会触发切换。例如：缺页异常。
3. 外设中断：当外设完成用户的请求时，会向CPU发送中断信号。

##### 系统调用

我们运行的程序基本都是运行在用户态，如果我们调用操作系统提供的系统态级别的子功能咋办呢？那就需要系统调用了！

也就是说在我们运行的用户程序中，凡是与系统态级别的资源有关的操作(如文件管理、进程控制、内存管理等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。

这些系统调用按功能大致可分为如下几类：

- 设备管理。完成设备的请求或释放，以及设备启动等功能。
- 文件管理。完成文件的读、写、创建及删除等功能。
- 进程控制。完成进程的创建、撤销、阻塞及唤醒等功能。·进程通信。完成进程之间的消息传递或信号传递等功能。
- 内存管理。完成内存的分配、回收以及获取作业占用内存区大小及地址等功能。

#### ==3、进程和线程==

##### 3.1 什么是进程？

进程是运行中的程序，程序从可执行文件被加载到内存中开始运行，即成为一个进程，**这是进程的本质**;

**操作系统会以进程为单位分配系统资源(CPU时间片、内存等资源)，进程是==资源分配==的最小单位**。

每个进程在内存中都有一个**进程控制块**对应，接受操作系统的调度，这是**进程调度的形式。**

###### 进程控制块

进程控制块，全称是 `Process Control Block`，它**保存了一切与进程运行状态相关的信息**，包括程序计数器、运行时栈、CPU寄存器值以及文件描述符等相关资源，它**代表进程接受操作系统的调度。**

**保存的位置：**

- 从物理上看，它可能存在于两个地方，一处是**内存**中，另一处是**外存中的 `swap` 空间**，当进程被挂起或者内存空间不够时，进程控制块可能会被置换到 `swap` 空间中去；
  - 从逻辑上看，它会**来往于各个队列**之中，例如正在运行时在ready queue中等待调度器的唤醒，例如在某个I/O的waiting queue中等待获得访问I/O资源的权限等等。

##### 3.2 什么是线程？

**线程，有时被称为轻量级进程(Lightweight Process，LWP)，是==操作系统调度(CPU调度)==执行的最小单位**。

##### 3.3 进程和线程的区别与联系

- **区别**

  - ```markdown
    1. 调度：线程作为调度和分配的基本单位，进程作为拥有资源的基本单位；
    2. 并发性：不仅进程之间可以并发执行，同一个进程的多个线程之间也可并发执行；
    3. **进程是拥有资源的一个独立单位，线程不拥有系统资源**，但可以访问隶属于进程的资源
    4. `最大的不同:
    `	基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响，而且多个线程会共享进程的堆和方法区(JDK1.8以后是元空间)资源。
     5. 线程执行开销小，但不利于资源的管理和保护；而进程正相反。
    ```
    
  - ![image-20210912155227282](https://gitee.com/yun-xiaojie/blog-image/raw/master/img/image-20210912155227282.png)

- **联系**

  - ```markdown
    1. 一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程；
    2. 资源分配给进程，同一进程的所有线程共享该进程的所有资源；
    3. 处理机分给线程，即真正在处理机上运行的是线程；
    4. 线程在执行过程中，需要协作同步。不同进程的线程间要利用消息通信的办法实现同步
    ```

##### 3.4 进程有哪几种状态

- **创建状态(new)：**进程正在被创建，尚未到就绪状态。
- **就绪状态(ready)：**进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。
- **运行状态(running)**：进程正在处理器上上运行(单核CPU下任意时刻只有一个进程处于运行状态)。
- **阻塞状态(waiting)：**又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待IO操作完成。即使处理器空闲，该进程也不能运行。
- **结束状态(terminated)：**进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。

##### ==3.5 进程间的通信方式==

###### **匿名管道(Pipes)**

用于具有==亲缘关系==的父子进程间或者兄弟进程之间的通信。

###### **有名管道(Names Pipes)**

匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循**先进先出(first in first out)**。有名管道以磁盘文件的方式存在，可以实现本机**任意两个进程**通信。

###### **信号(Signal)**

信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生；

###### **消息队列(Message Queuing)**

消息队列是消息的链表，具有特定的格式，存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是**先进先出**的原则。与管道(无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统)不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询，消息不一定要以先进先出的次序读取，也可以按消息的类型读取.比FIFO更有优势。**消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺。**

###### **信号量(Semaphores)**

信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。

###### **共享内存(Shared memory)**

​	共享内存就是允许两个不相关的进程访问同一个逻辑内存。共享内存是在两个正在运行的进程之间共享和传递数据的一种非常有效的方式。不同进程之间共享的内存通常安排为同一段物理内存。进程可以将同一段共享内存连接到它们自己的地址空间中，所有进程都可以访问共享内存中的地址，就好像它们是由用C语言函数 `malloc` 分配的内存一样。而如果某个进程向共享内存写入数据，所做的改动将立即影响到可以访问同一段共享内存的任何其他进程。

![img](https://gitee.com/yun-xiaojie/blog-image/raw/master/img/v2-195b0cf5f101ed8c11910fea9b77559e_720w.jpg)

多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。共享内存并未提供同步机制，也就是说，在第一个进程结束对共享内存的写操作之前，并无自动机制可以阻止第二个进程开始对它进行读取。所以我们通常需要用其他的机制来同步对共享内存的访问，如**互斥锁和信号量**等。可以说这是最有用的进程间通信方式。

**共享内存的使用**

```markdown
# 1. shmget函数
 	1.1 该函数用来创建共享内存
 	1.2 函数原型: int shmget(key_t key, size_t size, int shmflg)
```



###### **套接字(Sockets)**

此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持TCP/IP的网络通信的基本操作单元，可以看做是==不同主机之间的进程==进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。

##### 3.6 线程间的同步方式

线程同步是两个或三个共享资源的线程的并发执行。应该同步线程以避免关键的资源使用冲突。一般有以下三种线程同步的方式：

- **互斥量(Mutex)：**采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。
  因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如Java中的synchronized 关键词和各种Lock都是这种机制。
- **信号量(Semphares)：**它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量
- **事件(Event)：**Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操

##### ==3.7 进程的调度算法(CPU调度)==

为了确定首先执行哪个进程已经最后执行哪个进程以实现最大CPU利用率，定义了一些调度算法：

- **先到先服务(FCFS)调度算法：**从就绪队列中选择一个==最先进入该队列的进程==为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用CPU时再重新调度。**非抢占式**。
  - **优点**是公平，实现简单；
  - **缺点**是不利于短作业，因为短作业等待时间过长。
  - 该算法有利于 `CPU` 繁忙型(指需要大量 `CPU` 时间)的作业，不利于 `I/O` 繁忙型(指需要大量 `I/O` 时间)
- **短作业优先(SJF)的调度算法：**从就绪队列中选出一个==估计运行时间最短==的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用CPU时再重新调度。**抢占式**。
  - **优点** 可以有效降低作业的平均等待时间，提高系统吞吐量。
  - **缺点** 对长作业不利，周转时间与带权周转时间提升；未考虑作业的紧迫程度；时间长短是估计的，所以不一定会达到真正的短作业调度
- **时间片轮转调度算法：**时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法，又称RR(Round robin)调度。==每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间==。
  - **优点** 是兼顾长短作业；
  - **缺点** 是平均等待时间较长，上下文切换较费时。
  - 适用于分时系统。
- **多级反馈队列调度算法：**前面介绍的几种进程调度的算法都有一定的局限性。如**短进程优先的调度算法，仅照顾了短进程而忽略了长进程**。多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业(进程)迅速完成。因而它是目前被**公认的一种较好的进程调度算法，**UNIX操作系统采取的便是这种调度算法。
- **优先级调度：**为每个流程分配优先级，**首先执行具有最高优先级的进程，**依此类推。**具有相同优先级的进程以FCFS方式执行。**可以根据内存要求，时间要求或任何其他资源要求来确定优先级。

##### 3.8 协程

​	协程，是一种比线程更加轻量级的存在，**协程不是被操作系统内核所管理，而完全是由程序所控制(也就是在用户态执行)**。这样带来的好处就是性能得到了很大的提升，不会像线程切换那样消耗资源。

​	子程序，或者称为函数，在所有语言中都是层级调用，比如A调用B，B在执行过程中又调用了C，C执行完毕返回，B执行完毕返回，最后是A执行完毕。所以子程序调用是通过栈实现的，**一个线程就是执行一个子程序**。子程序调用总是一个入口，一次返回，调用顺序是明确的。而协程的调用和子程序不同。

​	**协程在子程序内部是可中断的，然后转而执行别的子程序，在适当的时候再返回来接着执行**。

```python
def A():
    print '1'
    print '2'
    print '3'

def B():
    print 'x'
    print 'y'
    print 'z'
```

假设由协程执行，在执行A的过程中，可以随时中断，去执行B，B也可能在执行过程中中断再去执行A，结果可能是：`1 2 x y 3 z`。

协程的特点在于是一个线程执行，那和多线程比，协程有何优势？

- **极高的执行效率**：因为==**子程序切换不是线程切换，而是由程序自身控制**==，因此，**==没有线程切换的开销==**，和多线程比，线程数量越多，协程的性能优势就越明显；
- **不需要多线程的锁机制**：因为只有一个线程，也不存在同时写变量冲突，**在协程中控制共享资源不加锁**，只需要判断状态就好了，所以执行效率比多线程高很多。

#### 4、内存管理介绍

##### 内存管理主要做什么

操作系统的内存管理主要**负责内存的分配与回收(malloc函数：申请内存，free函数：释放内存)**，另外**地址转换也就是将逻辑地址转换成相应的物理地址**等功能也是操作系统内存管理做的事情。

##### 内存管理机制

​		简单分为**连续分配管理方式**和**非连续分配管理方式**这两种。连续分配管理方式是指为一个用户程序分配一个连续的内存空间，常见的如**块式管理**。同样地，非连续分配管理方式允许一个程序使用的内存分布在离散或者说不相邻的内存中，常见的如**页式管理和段式管理**。

- **块式管理**：远古时代的计算机操系统的内存管理方式。将内存分为几个固定大小的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费了。这些在每个块中未被利用的空间，我们称之为碎片。
- **页式管理：**把主存分为大小相等且固定的一页一页的形式，页较小，相对相比于块式管理的划分力度更大，提高了内存利用率，减少了碎片。页式管理通过页表对应逻辑地址和物理地址。
- **段式管理：**页式管理虽然提高了内存利用率，但是页式管理其中的页实际并无任何实际意义。段式管理把主存分为一段段的，每一段的空间又要比一页的空间小很多。但是，最重要的是段是有实际意义的，每个段定义了一组逻辑信息，例如，有主程序段MAIN、子程序段X、数据段D及栈段S等。段式管理通过段表对应逻辑地址和物理地址。
- **段页式管理机制**：简单说来说，段页式管理机制就是把主存先分成若干段，每个段又分成若干页，也就是说**段页式管理机制**中段与段的内部都是离散的。

##### 快表和多级页表

分页内存管理中，很重要的两点式是：

- 虚拟地址到物理地址的转换要快；
- 解决虚拟地址空间大，页表也会很大的问题。

###### 快表

​		为了解决虚拟地址到物理地址的转换速度，操作系统在**页表方案**基础之上引入了**快表**来加速虚拟地址到物理地址的转换。我们可以把快表理解为一种特殊的高速缓冲存储器(Cache)，其中的内容是页表的一部分或者全部内容。作为页表的Cache，它的作用与页表相似，但是提高了访问速率。由于采用页表做地址转换，读写内存数据时CPU要访问两次主存。有了快表，有时只要访问一次高速缓冲存储器，一次主存，这样可加速查找并提高指令执行速度。

使用快表之后的地址转换流程是这样的：

1. 根据虚拟地址中的页号查快表；
2. 如果该页在快表中，直接从快表中读取相应的物理地址；
3. 如果该页不在快表中，就访问内存中的页表，再从页表中得到物理地址，同时将页表中的该映射表项添加到快表中；
4. 当快表填满后，又要登记新页时，就按照一定的淘汰策略淘汰掉快表中的一个页。

###### 多级页表

引入多级页表的主要目的是为了避免把全部页表一直放在内存中占用过多空间，特别是那些根本就不需要的页表就不需要保留在内存中。多级页表属于时间换空间的典型场景。

> 为了提高内存的空间性能，提出了多级页表的概念；但是提到空间性能是以浪费时间性能为基础的，因此为了补充损失的时间性能，提出了快表(即TLB)的概念。不论是快表还是多级页表实际上都利用到了程序的局部性原理。

##### 分页和分段机制的共同点和区别

共同点：

- 分页机制和分段机制都是为了提高内存利用率，减少内存碎片。
- 页和段都是离散存储的，所以两者都是离散分配内存的方式。但是，每个页和段中的内存是连续的。

区别：

- 页的大小是固定的，由操作系统决定；而段的大小不固定，取决于我们当前运行的程序。
- 分页仅仅是为了满足操作系统内存管理的需求，而段是逻辑信息的单位，在程序中可以体现为代码段，数据段，能够更好满足用户的需要。

##### 逻辑(内存)地址和物理地址

**物理地址**：加载到内存地址寄存器中的地址，**内存单元的真正地址**。在前端总线上传输的内存地址都是物理内存地址，编号从0开始一直到可用物理内存的最高端。这些数字被映射到实际的内存条上。物理地址是明确的、最终用在总线上的编号，不必转换，不必分页，也没有特权级检查。

**逻辑地址**：CPU所生成的地址。逻辑地址是内部和编程使用的、并不唯一。例如，你在进行C语言指针编程中，可以读取指针变量本身值(&操作)，实际上这个值就是逻辑地址，它是相对于你当前进程数据段的地址(偏移地址)，和绝对物理地址不相干。

##### CPU寻址、为什么使用虚拟地址空间

​		现代处理器使用的是一种称为**虚拟寻址(Virtual Addressing)**的寻址方式。**使用虚拟寻址，CPU需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。**实际上完成虚拟地址转换为物理地址转换的硬件是CPU中含有一个被称为**内存管理单元(Memory Management Unit，MMU)**的硬件。

![image-20210411164409248](https://gitee.com/yun-xiaojie/blog-image/raw/master/img/image-20210411164409248.png)

##### 为什么要有虚拟地址空间

没有虚拟地址空间的时候，**程序直接访问和操作的是物理内存**。但是这样会产生一些问题。

- 用户程序可以访问任意内存，寻址内存的每个字节，这样就很容易(有意或者无意)破坏操作系统，造成操作系统崩溃。
- 想要同时运行多个程序特别困难，比如你想同时运行一个微信和一个QQ音乐都不行。为什么呢？举个简单的例子：微信在运行的时候给内存地址1xxx赋值后，QQ音乐也同样给内存地址1xxx赋值，那么QQ音乐对内存的赋值就会覆盖微信之前所赋的值，这就造成了微信这个程序就会崩溃。

总结来说：**如果直接把物理地址暴露出来的话会带来严重问题，比如可能对操作系统造成伤害以及给同时运行多个程序造成困难。**

**通过虚拟地址寻址有以下优点：**

- 程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的大内存缓冲区。
- 程序可以使用一系列虚拟地址来访问大于可用物理内存的内存缓冲区。当物理内存的供应量变小时，内存管理器会将物理内存页(通常大小为4KB)保存到磁盘文件。数据或代码页会根据需要在物理内存与磁盘之间移动。
- 不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。

#### ==5、虚拟内存==

##### ==1、什么是虚拟内存(Virtual Memory)，即什么是虚拟存储器？==

​		基于**局部性原理**，**在程序装入时，可以将程序的一部分装入内存**，而将其他部分留在外存，就可以启动程序执行。由于外存往往比内存大很多，所以我们运行的软件的内存大小实际上是可以比计算机系统实际的内存大小大的。在程序执行过程中，**当所访问的信息不在内存时，由操作系统将所需要的部分调入内存，然后继续执行程序**。另一方面，**操作系统将内存中暂时不使用的内容换到外存上，从而腾出空间存放将要调入内存的信息**。这样，**计算机好像为用户提供了一个比实际内存大的多的存储器——虚拟存储器**。

​		之所以将其称为虚拟存储器，是因为**这种存储器实际上并不存在**，只是由于(对用户完全透明)，给用户的感觉是好像存在一个比实际物理内存大得多的存储器。**虚拟存储器的大小由计算机的地址结构决定**，并非是内存和外存的简单相加。**虚拟存储器有以下三个主要特征：**

- **多次性**: 是指**无需在作业运行时一次性地全部装入内存**，而是允许被分成多次调入内存运行。
- **对换性**: 是指**无需在作业运行时一直常驻内存**，而是允许在作业的运行过程中，进行换进和换出。
- **虚拟性**: 是指**从逻辑上扩充内存的容量**，使用户所看到的内存容量，远大于实际的内存容量。

> 实际上，我觉得虚拟内存同样是一种时间换空间的策略，你用CPU的计算时间，页的调入调出花费的时间，换来了一个虚拟的更大的空间来支持程序的运行。不得不感叹，程序世界几乎不是时间换空间就是空间换时间。

**虚拟内存的重要意义是它定义了一个==连续的==虚拟地址空间，**并且把**内存扩展到硬盘空间。**

##### 2、为什么要用虚拟内存?

```markdown
**
各种内存管理策略都是为了同时将多个进程保存在内存中以便允许多道程序设计。早期的内存分配方式都具有以下两个共同的特征：
1. 一次性
   > 作业必须一次性全部装入内存后，方能开始运行。这会导致两种情况发生：
   1.1 当作业很大，不能全部被装入内存时，将使该作业无法运行；
   1.2 当大量作业要求运行时，由于内存不足以容纳所有作业，只能使少数作业先运行，导致多道程序度的下降。
2. 驻留性
> 作业被装入内存后，就一直驻留在内存中，其任何部分都不会被换出，直至作业运行结束。运行中的进程，会因等待I/O而被阻塞，可能处于长期等待状态。
> 由以上分析可知，许多在程序运行中不用或暂时不用的程序(数据)占据了大量的内存空间，而一些需要运行的作业又无法装入运行，显然浪费了宝贵的内存资源。
**
**
> 因为早期的内存分配方法存在以下问题：
1. 进程地址空间不隔离。会导致数据被随意修改。
2. 内存使用效率低。
3. 程序运行的地址不确定。操作系统随机为进程分配内存空间，所以程序运行的地址是不确定的。
**
```

##### 3、使用虚拟内存的好处:

```markdown
**
1. 扩大地址空间。每个进程独占一个4G空间，虽然真实物理内存没那么多。
2. 内存保护：防止不同进程对物理内存的争夺和践踏，可以对特定内存地址提供写保护，防止恶意篡改。
3. 可以实现内存共享，方便进程通信。
4. 可以避免内存碎片，虽然物理内存可能不连续，但映射到虚拟内存上可以连续。
```

##### 4、使用虚拟内存的缺点：

```markdown
**
1. 虚拟内存需要额外构建数据结构，占用空间。
2. 虚拟地址到物理地址的转换，增加了执行时间。
3. 页面换入换出耗时。
4. 一页如果只有一部分数据，浪费内存。
```

##### 5、进程虚拟地址空间的分布

<img src="https://gitee.com/yun-xiaojie/blog-image/raw/master/img/49bf3b7726d50f6dc1b4164148418f8b.png">

```markdown
**
# 1. .text代码段
	也叫正文段，存放代码。
# 2. 数据段: 存放全局变量和静态变量
   # 2.1 .data:初始化数据段
   	存放已经初始化的
   # 2.2 .bss:未初始化数据段
    存放未初始化的
# 3. 堆和栈
   # 3.1 分配方式
   > 栈：由编译器自动分配释放，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。
   > 堆: 一般由程序员分配释放，它的分配方式类似于链表。
   # 3.2 申请后系统的响应
   > 栈：只要所申请的空间小于栈的剩余空间，则系统为程序分配内存，否则栈溢出。
   >  堆：操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，遍历该链表，找出第一个大于所申请空间的节点，然后将其从链表中删除并分配，如果没用完，则系统会把多余的重新放回到链表中。
   # 3.3 申请大小的限制
   > 栈：栈是高地址向低地址扩展的连续内存，栈的大小一般是2M;
   > 堆：堆是低地址向高地址扩展的不连续内存，堆的大小与计算机有效的虚拟内存有关系。
   # 4.4 申请效率
   > 栈：由系统自动分配，速度较快；
   > 堆：速度慢，容易产生内存碎片；
```

##### 6、局部性原理

很早就有人指出，程序在执行的时候往往呈现局部性规律，也就是在某个较短的时间段内，程序执行局限于某一小部分，程序访问的存储空间也局限于某个区域。

局部性体现在以下两个方面：

1. **时间局部性：**如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。**产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。**
2. **空间局部性：**一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，**这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。**

时间局部性是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。空间局部性通常是使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上就是建立了“内存——外存”的两级存储器的结构，利用局部性原理实现高速缓存。

##### 8、虚拟内存的技术实现

**虚拟内存**的实现需要建立在离散分配的内存管理方式的基础上。虚拟内存的实现方式有以下三种方法：

1. **请求分页存储管理：**建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。请求分页存储管理系统中，在作业开始运行之前，仅装入当前要执行的部分段即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存，同时操作系统也可以将暂时不用的页面置换到外存中。
2. **请求分段存储管理：**建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段；当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。
3. **请求段页式存储管理**

不管哪种方式，都需要有一定的硬件支持。一般需要的支持有以下几个方面：

- **一定容量的内存和外存。**

- **页表机制(或段表机制)，作为主要的数据结构。**

- **中断机构，当用户程序要访问的部分尚未调入内存，则产生中断。**

- **地址变换机构，逻辑地址到物理地址的变换。**

###### 8.1、请求分页管理方式实现虚拟内存

**请求分页系统**建立在基本分页系统基础之上，为了支持虚拟存储器功能而**增加了请求调页功能和页面置换功能**。**请求分页是目前最常用的一种实现虚拟存储器的方法**。

在请求分页系统中，**只要求将当前需要的一部分页面装入内存，便可以启动作业运行。**在作业执行过程中，当所要访问的页面不在内存时，**再通过调页功能将其调入，同时还可以通过置换功能将暂时不用的页面换出到外存上，**以便腾出内存空间。

为了实现请求分页，系统必须提供一定的硬件支持。除了需要一定容量的内存及外存的计算机系统，还需要有**页表机制、缺页中断机构和地址变换机构**。

- **页表机制**

  **请求分页系统的页表机制不同于基本分页系统，请求分页系统在一个作业运行之前不要求全部一次性调入内存，因此在作业的运行过程中，必然会出现要访问的页面不在内存的情况**，如何发现和处理这种情况是请求分页系统必须解决的两个基本问题。为此，**在请求页表项中增加了四个字段**

  - 状态位P：用于指示该页是否已调入内存，供程序访问时参考。
  - 访问字段A：用于记录本页在一段时间内被访问的次数，或记录本页最近已有多长时间未被访问，供置换算法换出页面时参考。
  - 修改位M：标识该页在调入内存后是否被修改过。
  - 外存地址：用于指出该页在外存上的地址，通常是物理块号，供调入该页时参考。

- **缺页中断机构**

  在请求分页系统中，每当所要访问的页面不在内存时，便产生一个缺页中断，请求操作系统将所缺的页调入内存。此时应将缺页的进程阻塞(调页完成唤醒)，如果内存中有空闲块，则分配一个块，将要调入的页装入该块，并修改页表中相应页表项，若此时内存中没有空闲块，则要淘汰某页(若被淘汰页在内存期间被修改过，则要将其写回外存)。

  缺页中断作为中断同样要经历，诸如保护CPU环境、分析中断原因、转入缺页中断处理程序、恢复CPU环境等几个步骤。但与一般的中断相比，它有以下两个明显的区别：

  ​	在指令执行期间产生和处理中断信号，而非一条指令执行完后，属于内部中断。

  ​	一条指令在执行期间，可能产生多次缺页中断。

- **地址变化机构**

  - 请求分页系统中的地址变换机构，是在分页系统地址变换机构的基础上，为实现虚拟内存，又增加了某些功能而形成的。

    ![img](https://gitee.com/yun-xiaojie/blog-image/raw/master/img/20180525153826791)

  在进行地址变换时，先检索快表：

  - 若找到要访问的页，便修改页表项中的访问位(写指令则还须重置修改位)，然后利用页表项中给出的物理块号和页内地址形成物理地址。
  - 若未找到该页的页表项，应到内存中去查找页表，再对比页表项中的状态位P，看该页是否已调入内存，未调入则产生缺页中断，请求从外存把该页调入内存。

- **页面分配策略**

  - **驻留集大小**
    - 对于分页式的虚拟内存，在准备执行时，不需要也不可能把一个进程的所有页都读取到主存，因此，操作系统必须决定读取多少页。也就是说，给特定的进程分配多大的主存空间，这需要考虑以下几点：
      - 分配给一个进程的存储量越小，在任何时候驻留在主存中的进程数就越多，从而可以提高处理机的时间利用效率。
      - 如果一个进程在主存中的页数过少，尽管有局部性原理，页错误率仍然会相对较高。
      - 如桌页数过多，由于局部性原理，给特定的进程分配更多的主存空间对该进程的错误率没有明显的影响。
    - 基于这些因素，现代操作系统通常采用三种策略
      - **固定分配局部置换** 它为每个进程分配一定数目的物理块，在整个运行期间都不改变。若进程在运行中发生缺页，则只能从该进程在内存中的页面中选出一页换出，然后再调入需要的页面。实现这种策略难以确定为每个进程应分配的物理块数目：太少会频繁出现缺页中断，太多又会使CPU和其他资源利用率下降。
      - **可变分配全局置换** 这是最易于实现的物理块分配和置换策略，为系统中的每个进程分配一定数目的物理块,操作系统自身也保持一个空闲物理块队列。当某进程发生缺页时，系统从空闲物理块队列中取出一个物理块分配给该进程，并将欲调入的页装入其中。
      - **可变分配局部置换 **它为每个进程分配一定数目的物理块，当某进程发生缺页时，只允许从该进程在内存的页面中选出一页换出，这样就不会影响其他进程的运行。如果进程在运行中频繁地缺页，系统再为该进程分配若干物理块，直至该进程缺页率趋于适当程度； 反之，若进程在运行中缺页率特别低，则可适当减少分配给该进程的物理块。
  - **调入页面的时机**
    - 为确定系统将进程运行时所缺的页面调入内存的时机，可釆取以下两种调页策略：
      - **预调页策略。** 根据局部性原理，一次调入若干个相邻的页可能会比一次调入一页更高效。但如果调入的一批页面中大多数都未被访问，则又是低效的。所以就需要釆用以预测为基础的预调页策略，将预计在不久之后便会被访问的页面预先调入内存。但目前预调页的成功率仅约50%。故这种策略主要用于进程的首次调入时，由程序员指出应该先调入哪些页。
      - **请求调页策略。** 进程在运行中需要访问的页面不在内存而提出请求，由系统将所需页面调入内存。由这种策略调入的页一定会被访问，且这种策略比较易于实现，故在目前的虚拟存储器中大多釆用此策略。它的缺点在于每次只调入一页，调入调出页面数多时会花费过多的I/O开销。
  - **从何处调入页面**
    - 请求分页系统中的外存分为两部分：用于存放文件的文件区和用于存放对换页面的对换区。对换区通常是釆用连续分配方式，而文件区釆用离散分配方式，故对换区的磁盘I/O速度比文件区的更快。这样从何处调入页面有三种情况：

      1.      系统拥有足够的对换区空间：可以全部从对换区调入所需页面，以提髙调页速度。为此，在进程运行前，需将与该进程有关的文件从文件区复制到对换区。

      2.      系统缺少足够的对换区空间：凡不会被修改的文件都直接从文件区调入；而当换出这些页面时，由于它们未被修改而不必再将它们换出。但对于那些可能被修改的部分，在将它们换出时须调到对换区，以后需要时再从对换区调入。

      3.      UNIX方式：与进程有关的文件都放在文件区，故未运行过的页面，都应从文件区调入。曾经运行过但又被换出的页面，由于是被放在对换区，因此下次调入时应从对换区调入。进程请求的共享页面若被其他进程调入内存，则无需再从对换区调入。

  

  **还有信息没记录**https://blog.csdn.net/u014590757/article/details/80452618

  ![image-20211010213818768](https://gitee.com/yun-xiaojie/blog-image/raw/master/img/image-20211010213818768.png)



###### 8.2、分页存储管理和请求分页存储管理的不同

​	请求分页存储管理建立在分页管理之上。他们的**根本区别是是否将程序全部所需的全部地址空间都装入主存，这也是==请求分页存储管理可以提供虚拟内存的原因==**。

​	它们之间的根本区别在于是否将作业的全部地址空间同时装入主存。**==请求分页存储管理不要求将作业全部地址空间同时装入主存==。**基于这一点，**==请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存==。**

不管是上面那种实现方式，我们一般都需要：

- 一定容量的内存和外存：在载入程序的时候，只需要将程序的一部分装入内存，而将其他部分留在外存，然后程序就可以执行了；
- **缺页中断：**如果**需执行的指令或访问的数据尚未在内存**(称为缺页或缺段)，则由处理器通知操作系统将相应的页面或段**调入到内存，**然后继续执行程序；
- **虚拟地址空间：**逻辑地址到物理地址的变换。

##### 9、页面置换算法

地址映射过程中，若在页面中发现所要访问的页面不在内存中，则发生缺页中断

> **缺页中断：**就是要访问的**页**不在内存，需要操作系统将其调入主存后再进行访问。在这个时候，被内存映射的文件实际上成了一个分页交换文件。

当发生缺页中断时，如果当前内存中并没有空闲的页面，操作系统就必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。用来**选择淘汰哪一页的规则叫做页面置换算法**，我们可以把页面置换算法看成是淘汰页面的规则。

- **OPT页面置换算法(最佳页面置换算法)：**最佳(Optimal，OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面，这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若千页面中哪个是未来最长时间内不再被访问的，**因而该算法无法实现。一般作为衡量其他置换算法的方法。**
- **FIFO(First In First Out)页面置换算法(先进先出页面置换算法)**：总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。
- **LRU(Least Currently Used)页面置换算法(最近最久未使用页面置换算法)：**LRU算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间T，当须淘汰一个页面时，选择现有页面中其T值最大的，即最近最久未使用的页面予以淘汰。
- **LFU(Least Frequently Used)页面置换算法(最少使用页面置换算法)：**该置换算法选择在之前时期使用最少的页面作为淘汰页。

##### 虚拟地址转换成物理地址

https://blog.csdn.net/qq_40780910/article/details/81570439

#### 6、CPU与磁盘的交互

​		CPU与磁盘是通过I/O总线关联到一起的，CPU操作磁盘是通过一种叫做存储器映射I/O的技术来实现的。这种技术会在两者交互时单独划分出一块地址，每个这样的地址就称作I/O端口，然后磁盘就会映射到一个或多个I/O端口，用以绑定地址。

​		CPU依次向磁盘发送指令：磁盘读(或写)命令、逻辑块号、主存地址。

​		发送完指令后，CPU会转去执行其它任务(为了提高效率)，磁盘则会将逻辑块号转换成对应的盘片、磁道、扇区组成的三元组，从而定位到了数据所在的扇区。之后磁盘会采用DMA(直接存储器访问技术，其不需要CPU干预)传送数据到CPU指定的主存地址。

​		最后，磁盘传送完毕后，会直接发送一个中断信号给CPU芯片的一个外部引脚，把CPU“召唤”回来重新执行先前未完成的任务。

##### 6.1、 CPU拿到地址后取得数据的过程？

​		CPU的运行原理就是：**控制单元**在时序脉冲的作用下，将**程序计数器**里所指向的指令地址送到地址总线上去，然后CPU将这个地址里的指令读到**指令寄存器**进行译码。对于执行指令过程中所需要用到的数据，会将数据地址也送到地址总线，然后CPU把数据读到CPU的**内部存储单元**(就是内部寄存器)暂存起来，最后命令**运算单元**对数据进行处理加工。这个过程不断重复，直到程序结束。

#### 7、IO多路复用

参考博客：[**地址**](https://juejin.cn/post/6882984260672847879)。

```markdown
# 定义：
- IO多路复用是一种同步IO模型，实现一个线程可以监视多个文件句柄；
- 一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；
- 没有文件句柄就绪时会阻塞应用程序，交出cpu。
** `多路是指网络连接，复用指的是同一个线程` **

# 为什么有IO多路复用？
没有IO多路复用机制时，有 ** `BIO、NIO` ** 两种实现方式，但有一些问题：
1. ** 同步阻塞(BIO) **
	1.1 服务端采用单线程，当 accept 一个请求后，在 recv 或 send 调用阻塞时，将无法 accept 其他请求(必须等上一个请求处理 recv 或 send 完 )  **`(无法处理并发)`**
	1.2 服务端采用多线程，当 accept 一个请求后，开启线程进行 recv，可以完成并发处理，但随着请求数增加需要增加系统线程，大量的线程占用很大的内存空间，并且线程切换会带来很大的开销，10000个线程真正发生读写实际的线程数不会超过20%，每次accept都开一个线程也是一种资源浪费。
2. ** 同步非阻塞(NIO) **
	- 服务器端当 accept 一个请求后，加入 fds 集合，每次轮询一遍 fds 集合 recv (非阻塞)数据，没有数据则立即返回错误，每次轮询所有 fd (包括没有发生读写实际的 fd)会很浪费 CPU。
```

##### 底层原理(`select`、`poll`、`epoll`)

IO多路复用有三种实现：`select`、`poll`、`epoll`。

###### **`select`**

它仅仅知道了，有 `I/O` 事件发生了，却并不知道是哪几个流(可能有一个，多个，甚至全部)，我们只能**无差别轮询所有流**，找出能读出数据或者写入数据的流，对他们进行操作。所以 **select具有`O(n)`的无差别的轮询复杂度**，同时处理的流越多，无差别轮询时间就越长。

**缺点：**

`select `本质上是通过设置或者检查存放 `fd(文件描述符)` 标志位的数据结构来进行下一步处理。这样所带来的缺点是：

- 单个进程所打开的 `FD` 是有**限制**的，通过 **`FD_SETSIZE`** 设置，默认 $1024$ ;
  - 文件描述符要限定在 $1024$ 的原因：
    - 一个进程所打开的最大文件描述符也是小于 $1024$，设置为小于 $1024$ 是比较合理的。
    - `select` 采用的是轮询处理的方式，处理过多的描述符开销比较大，会大大降低性能。
  - 文件描述符是如何存储的？
    - `select` 的文件描述符是由 `d_set` 类型管理的，`d_set` 类型实际是一个 `unsigned long` 型、大小为$32$ 的数组。一共有 $32$ 位字节、$32\times32=1024$ 位(bit)，每一位都可以表示一个描述符，共支持 $1023$ 个文件描述符。比如：将文件描述符 $4$ 加载到 `read fds` 中，实际 $4$ 文件描述符的存储方式就是$fds\_bits[0] = 0b0000000000001000$。
- 每次调用 `select`，都需要把 `fd` 集合**从用户态拷贝到内核态**，这个开销在 `fd` 很多时会很大；
- 对 `socket` 扫描时是线性扫描，采用轮询的方法，效率较低(高并发)

###### **`poll`**

`poll` 本质上和 `select` 没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个 `fd` 对应的设备状态, **但是它没有最大连接数的限制**，原因是它是基于链表来存储的.

**缺点：**

**它没有最大连接数的限制**，原因是它是基于链表来存储的，但是同样有缺点：

- 每次调用 `poll` ，都需要把 `fd` 集合从用户态拷贝到内核态，这个开销在 `fd` 很多时会很大；
- 对 `socket` 扫描是线性扫描，采用轮询的方法，效率较低(高并发时)

###### **`epoll`**

**`epoll `可以理解为 `event poll`**，不同于忙轮询和无差别轮询，`epoll `会把哪个流发生了怎样的 `I/O` 事件通知我们。所以我们说`epoll`实际上是**事件驱动(每个事件关联 `fd`)**的，此时我们对这些流的操作都是有意义的。(复杂度降低到了`O(1)`)

**优点：**

- 没有最大并发连接的限制，能打开的 `FD` 的上限远大于 $1024$(1G的内存上能监听约10万个端口)；
- 效率提升，不是轮询的方式，不会随着 `FD` 数目的增加效率下降。只有活跃可用的 `FD` 才会调用 callback 函数；即 **`Epoll` 最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关**，因此在实际的网络环境中，`Epoll` 的效率就会远远高于 `select `和 `poll`；
- 内存拷贝，利用 `mmap()` 文件映射内存加速与内核空间的消息传递；即 `epoll `使用 `mmap `减少复制开销

**缺点：**只能工作在 Linux 下。

IO多路复用在Linux下包括了三种，[select](https://link.zhihu.com/?target=http%3A//man7.org/linux/man-pages/man2/select.2.html)、[poll](https://link.zhihu.com/?target=http%3A//man7.org/linux/man-pages/man2/poll.2.html)、[epoll](https://link.zhihu.com/?target=http%3A//man7.org/linux/man-pages/man7/epoll.7.html)，抽象来看，他们功能是类似的，但具体细节各有不同：首先都会对一组文件描述符进行相关事件的注册，然后阻塞等待某些事件的发生或等待超时。更多细节详见下面的 "具体怎么用"。IO多路复用都可以关注多个文件描述符，但对于这三种机制而言，不同数量级文件描述符对性能的影响是不同的

##### 7.1、水平触发,边缘触发

`epoll` 也是实现` I/O` 多路复用的一种方法，为了深入了解 `epoll` 的原理，我们先来看下 `epoll` 水平触发(level trigger，LT，LT为 `epoll` 的默认工作模式)与边缘触发(edge trigger，ET)两种工作模式。

使用脉冲信号来解释LT和ET可能更加贴切。Level是指信号只需要处于水平，就一直会触发；而edge则是指信号为上升沿或者下降沿时触发。说得还有点玄乎，我们以生活中的一个例子来类比LT和ET是如何确定**读操作**是否就绪的。

**水平触发**

儿子：妈妈，我收到了500元的压岁钱。
妈妈：嗯，省着点花。
儿子：妈妈，我今天花了200元买了个变形金刚。
妈妈：以后不要乱花钱。
儿子：妈妈，我今天买了好多好吃的，还剩下100元。
妈妈：用完了这些钱，我可不会再给你钱了。
儿子：妈妈，那100元我没花，我攒起来了
妈妈：这才是明智的做法！
儿子：妈妈，那100元我还没花，我还有钱的。
妈妈：嗯，继续保持。
儿子：妈妈，我还有100元钱。
妈妈：…
只要儿子一直有钱，他就一直会向他的妈妈汇报。**LT模式下，只要内核缓冲区中还有未读数据，就会一直返回描述符的就绪状态，即不断地唤醒应用进程。**在上面的例子中，儿子是缓冲区，钱是数据，妈妈则是应用进程了解儿子的压岁钱状况(读操作)。

**边缘触发**

儿子：妈妈，我收到了500元的压岁钱。
妈妈：嗯，省着点花。
(儿子使用压岁钱购买了变形金刚和零食。)
儿子：
妈妈：儿子你倒是说话啊？压岁钱呢？

这个就是ET模式，儿子只在第一次收到压岁钱时通知妈妈，接下来儿子怎么把压岁钱花掉并没有通知妈妈。即儿子从没钱变成有钱，需要通知妈妈，接下来钱变少了，则不会再通知妈妈了。**在ET模式下， 缓冲区从不可读变成可读，会唤醒应用进程，缓冲区数据变少的情况，则不会再唤醒应用进程。**

我们再详细说明LT和ET两种模式下对读写操作是否就绪的判断。

###### 7.1.1 水平触发

- **对于读操作：**只要缓冲区不为空，LT 模式返回读就绪。
- **对于写操作：**只要缓冲区不为空，LT 模式返回写就绪。

###### 7.1.2 边缘触发

- **对于读操作：**
  - 缓冲区由不可读变为可读的时候，即缓冲区变为非空的时候。
  - 当有新数据到达时，即缓冲区的待读数据变多的时候。
  - 当缓冲区有数据可读，且应用进程对相应的描述符进行`EPOLL_CTL_MOD` 修改`EPOLLIN`事件时。
- **对于写操作：**
  - 缓冲区由不可写变为可写时
  - 当有旧数据被发送走，即缓冲区中的内容变少的时候
  - 当缓冲区有空间可写，且应用进程对相应的描述符进行`EPOLL_CTL_MOD` 修改`EPOLLOUT`事件时。

##### 7.2、优缺点

###### 7.2.1 水平触发

优点：当进行socket通信的时候，保证了数据的完整输出，进行IO操作的时候，如果还有数据，就会一直的通知你。

缺点：由于只要还有数据，内核就会不停的从内核空间转到用户空间，所有占用了大量内核资源，试想一下当有大量数据到来的时候，每次读取一个字节，这样就会不停的进行切换。内核资源的浪费严重。效率来讲也是很低的。

###### 7.2.2 垂直触发

优点：每次内核只会通知一次，大大减少了内核资源的浪费，提高效率。

缺点：不能保证数据的完整。不能及时的取出所有的数据。

应用场景：处理大数据。使用non-block模式的socket。

**总结：**

如果我们用水平触发不用担心数据有没有读完因为下次epoll返回时，没有读完的socket依然会被返回，但是要注意这种模式下的写事件，因为是水平触发，每次socket可写时epoll都会返回，当我们写的数据包过大时，一次写不完，要多次才能写完或者每次socket写都写一个很小的数据包时，每次写都会被epoll检测到，因此长期关注socket写事件会无故cpu消耗过大甚至导致cpu跑满，所以在水平触发模式下我们一般不关注socket可写事件而是通过调用socket write或者send api函数来写socket，说到这我们可以看到这种模式在效率上是没有边缘触发高的，因为每个socket读或者写可能被返回两次甚至多次，所以有时候我们也会用到边缘触发；

但是这种模式下在读数据的时候一定要注意，因为如果一次可写事件我们没有把数据读完，如果没有读完，在socket没有新的数据可读时epoll就不回返回了，只有在新的数据到来时，我们才能读取到上次没有读完的数据。



#### 8、定义全局变量后，fork子进程，子进程能感知到全局变量吗？父进程和子进程读到的变量地址一样吗

​	`fork()` 函数用于从一个已经存在的进程内创建一个新的进程，新的进程称为“子进程”，相应地称创建子进程的进程为“父进程”。使用 `fork()` 函数得到的子进程是父进程的复制品，子进程完全复制了父进程的资源，包括进程上下文、代码区、数据区、堆区、栈区、内存信息、打开文件的文件描述符、信号处理函数、进程优先级、进程组号、当前工作目录、根目录、资源限制和控制终端等信息，而子进程与父进程的区别有进程号、资源使用情况和计时器等。

​		

​	对于父子进程，假设我们在代码中定义了一个全局变量 `g_test =1` ; 当使用 `fork()` 系统调用时，子进程使用的页目录和页表是复制父进程的，所有此时父子进程共享所有数据(当然 `fork()` 的返回值不一样，信号位图也不一样)，所以这个时候父子进程读取全局变量 `g_test` 的值以及地址都是一样的，因为 `g_test` 是被共享的，``g_test` 这块内存在两个进程的页表中都映射了同一块物理内存。

​	如果之后父子进程中有一方执行了写操作(比如子进程进行 `g_test++` )，子进程在查找页表时会发现这页内存是共享的(内存引用计数大于1)，这时候就触发了写时复制COW机制，系统会为子进程申请一页新内存，并拷贝父进程g_test所在内存页的数据到子进程新申请的内存中，并更新子进程中页表信息( `g_test` 所对应的线性地址映射到新内存)，然后再执行 `g_test++` 操作。

​	此时父子进程访问 `g_test` 时，虽然变量名一样，变量的线性地址一样(``printf(&g_test)``)，但是他们的值是不一样的，因为这个时候，父子进程的 `g_test` 被映射到了不同的物理内存中。

#### 9、零拷贝(Zero-copy)

##### 9.1、传统的数据传输方式

在互联网时代，从某台机器将一份数据（比如一个文件）通过网络传输到另外一台机器，是再平常不过的事情了。如果按照一般的思路，用Java语言来描述发送端的逻辑，大致如下。

```java
Socket socket = new Socket(HOST, PORT);
InputStream inputStream = new FileInputStream(FILE_PATH);
OutputStream outputStream = new DataOutputStream(socket.getOutputStream());

byte[] buffer = new byte[4096];
while (inputStream.read(buffer) >= 0) {
    outputStream.write(buffer);
}

outputStream.close();
socket.close();
inputStream.close();
```

看起来当然是很简单的。但是如果我们深入到操作系统的层面，就会发现实际的微观操作要更复杂，具体来说有以下步骤：

- `JVM` 向 `OS` 发出 `read()` 系统调用，触发上下文切换，从用户态切换到内核态。
- 从外部存储（如硬盘）读取文件内容，通过`直接内存访问（DMA）`存入内核地址空间的缓冲区。
- 将数据从内核缓冲区拷贝到用户空间缓冲区，`read()` 系统调用返回，并从内核态切换回用户态。
- `JVM` 向 `OS` 发出 `write()` 系统调用，触发上下文切换，从用户态切换到内核态。
- 将数据从用户缓冲区拷贝到内核中与目的地 `Socket` 关联的缓冲区。
- 数据最终经由 `Socket` 通过 `DMA` 传送到硬件（如网卡）缓冲区，`write()` 系统调用返回，并从内核态切换回用户态。

传统方法的时序图如下图所示：

![image-20211122195820480](https://i.loli.net/2021/11/22/c3tAE1Q2hoDZmzk.png)

到了这一步，你是否觉得简单的代码逻辑下隐藏着很累赘的东西了？事实也确实如此，这个过程一共发生了4次上下文切换（严格来讲是模式切换），并且数据也被来回拷贝了4次。如果忽略掉系统调用的细节，整个过程可以用下面的两张简图表示。

![image-20211122195851014](https://i.loli.net/2021/11/22/MDS61P2jeXgpcmu.png)



![image-20211122201238604](https://i.loli.net/2021/11/22/n45DlLA3HEewQVT.png)



我们都知道，上下文切换是CPU密集型的工作，数据拷贝是I/O密集型的工作。如果一次简单的传输就要像上面这样复杂的话，效率是相当低下的。零拷贝机制的终极目标，就是消除冗余的上下文切换和数据拷贝，提高效率。

##### 9.2、零拷贝的数据传输方式

###### 基础的零拷贝机制

通过上面的分析可以看出，第2、3次拷贝（也就是从内核空间到用户空间的来回复制）是没有意义的，数据应该可以直接从内核缓冲区直接送入 `Socket` 缓冲区。零拷贝机制就实现了这一点。不过零拷贝需要由操作系统直接支持，不同 `OS` 有不同的实现方法。大多数Unix-like系统都是提供了一个名为 `sendfile()` 的系统调用，有这样的描述：

> sendfile() copies data between one file descriptor and another.
>  Because this copying is done within the kernel, sendfile() is more efficient than the combination of read(2) and write(2), which would require transferring data to and from user space.
>
> 
>
> `sendfile()` 在一个文件描述符和另一个文件描述符之间拷贝数据。由于这种复制是在内核中完成的，`sendfile()` 比`read()` 和 `write()` 的组合更有效，后者需要在用户空间中传输数据。

下面是零拷贝机制下，数据传输的时序图：

![image-20211122200446829](https://i.loli.net/2021/11/22/k2bImY38E4rs9Jp.png)

可见确实是消除了从内核空间到用户空间的来回复制，因此 `zero-copy` 这个词实际上是站在**内核**的角度来说的，并不是完全不会发生任何拷贝。

在 `Java NIO` 包中提供了零拷贝机制对应的API，即`FileChannel.transferTo()` 方法。不过 `FileChannel` 类是抽象类，`transferTo()` 也是一个抽象方法，因此还要依赖于具体实现。`FileChannel` 的实现类并不在JDK本身，而位于`sun.nio.ch.FileChannelImpl` 类中，零拷贝的具体实现自然也都是 `native` 方法。

传统方式的信息发送逻辑可改为如下方式：

```java
SocketAddress socketAddress = new InetSocketAddress(HOST, PORT);
SocketChannel socketChannel = SocketChannel.open();
socketChannel.connect(socketAddress);

File file = new File(FILE_PATH);
FileChannel fileChannel = new FileInputStream(file).getChannel();
fileChannel.transferTo(0, file.length(), socketChannel);

fileChannel.close();
socketChannel.close();
```

借助 `transferTo()` 方法的话，整个过程就可以用下面的简图表示了。

![image-20211122200931645](https://i.loli.net/2021/11/22/6tzQceAXyTojWZN.png)

![image-20211122200946965](https://i.loli.net/2021/11/22/FMNauXWQ6fetRSr.png)

可见，不仅拷贝的次数变成了3次，上下文切换的次数也减少到了2次，效率比传统方式高了很多。但是它还并非完美状态，下面看一看让它变得更优化的方法。

###### 对Scatter/Gather的支持

在“基础”零拷贝方式的时序图中，有一个“write data to target socket buffer”的回环，在框图中也有一个从`Read buffer` 到 `Socket buffer` 的大箭头。这是因为在一般的 `Block DMA` 方式中，源物理地址和目标物理地址都得是连续的，所以一次只能传输物理上连续的一块数据，每传输一个块发起一次中断，直到传输完成，所以必须要在两个缓冲区之间拷贝数据。

而 `Scatter/Gather DMA` 方式则不同，会预先维护一个物理上不连续的块描述符的链表，描述符中包含有数据的起始地址和长度。传输时只需要遍历链表，按序传输数据，全部完成后发起一次中断即可，效率比 `Block DMA` 要高。也就是说，硬件可以通过 `Scatter/Gather DMA` 直接从内核缓冲区中取得全部数据，不需要再从内核缓冲区向 `Socket` 缓冲区拷贝数据。因此上面的时序图还可以进一步简化：

![image-20211122201417926](https://i.loli.net/2021/11/22/83ecwk1y2StqVAn.png)

它的流程框图如下：

![image-20211122201455447](https://i.loli.net/2021/11/22/MklidcgBHyJZWN6.png)

######  对内存映射（mmap）的支持

上面讲的机制看起来一切都很好，但它还是有个缺点：如果我想在传输时修改数据本身，就无能为力了。不过，很多操作系统也提供了内存映射机制，对应的系统调用为 `mmap()/munmap()`。通过它可以将文件数据映射到内核地址空间，直接进行操作，操作完之后再刷回去。其对应的简要时序图如下：

![image-20211122201543479](https://i.loli.net/2021/11/22/CoNAFstwmjP1n2E.png)

当然，天下没有免费的午餐，上面的过程仍然会发生4次上下文切换。另外，它需要在快表（TLB）中始终维护着所有数据对应的地址空间，直到刷写完成，因此处理缺页的overhead也会更大。在使用该机制时，需要权衡效率。

`NIO` 框架中提供了 `MappedByteBuffer` 用来支持 `mmap` 。它与常用的 `DirectByteBuffer` 一样，都是在堆外内存分配空间。相对地，`HeapByteBuffer` 在堆内内存分配空间。

[零拷贝在大数据方面的应用](https://www.jianshu.com/p/193cae9cbf07)
